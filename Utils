# This file contains all of the code for loading in the data

import numpy as np
import h5py
from sklearn import StandardScaler


def loading_all_data(filenames):

    # for the first dataset
    
    # I want to stick these into tensors 
    
    with h5py.File(filenames[0], 'r') as hdf:
        
        # Development set
        W_dev = np.array(hdf.get('W_dev')).astype(np.float32)             # W
        X_s_dev = np.array(hdf.get('X_s_dev')).astype(np.float32)         # X_s
        Y_dev = np.array(hdf.get('Y_dev')).astype(np.float32)
        A_dev = np.array(hdf.get('A_dev')).astype(np.float32)  
        A_dev = np.delete(A_dev, 1, 1)

        # Test set
        W_test = np.array(hdf.get('W_test')).astype(np.float32)           # W
        X_s_test = np.array(hdf.get('X_s_test')).astype(np.float32)       # X_s
        Y_test = np.array(hdf.get('Y_test')).astype(np.float32)           # RUL  
        A_test = np.array(hdf.get('A_test')).astype(np.float32)           # Auxiliary
        A_test = np.delete(A_test, 1, 1)
    
    print("N-CMAPSS_DS01.h5 Dataset Loaded")
    num = 50
    
    for file in filenames[1:]: 
    
        # Load data
        with h5py.File(file, 'r') as hdf:
            
            # Development set
            W_dev = np.concatenate((W_dev, np.array(hdf.get('W_dev')).astype(np.float32)), axis=0)             # W
            X_s_dev = np.concatenate((X_s_dev, np.array(hdf.get('X_s_dev')).astype(np.float32)), axis=0)         # X_s
            Y_dev = np.concatenate((Y_dev, np.array(hdf.get('Y_dev')).astype(np.float32)), axis=0)             # RUL  
            
            Aux_raw = np.array(hdf.get('A_dev')).astype(np.float32)
            Aux_raw[:, 0] = Aux_raw[:, 0] + num
            Aux_raw = np.delete(Aux_raw, 1, 1)
            A_dev = np.concatenate((A_dev, Aux_raw), axis=0)    # Auxiliary
            
            # Test set
            W_test = np.concatenate((W_test, np.array(hdf.get('W_test')).astype(np.float32)), axis=0)           # W
            X_s_test = np.concatenate((X_s_test, np.array(hdf.get('X_s_test')).astype(np.float32)), axis=0)       # X_s
            Y_test = np.concatenate((Y_test, np.array(hdf.get('Y_test')).astype(np.float32)), axis=0)           # RUL  
            
            Aux_raw_test = np.array(hdf.get('A_test')).astype(np.float32)
            Aux_raw_test[:, 0] = Aux_raw_test[:, 0] + num           
            Aux_raw_test = np.delete(Aux_raw_test, 1, 1)
            
            A_test = np.concatenate((A_test, Aux_raw_test), axis=0)                           # Auxiliary

            
        num += 50
        print(file, ' dataset loaded')
        
    # pulling out the unit numbers before scaling 
    
    train_units = pd.DataFrame(A_dev[:, 0].reshape(-1,1))
    test_units = pd.DataFrame(A_test[:, 0].reshape(-1,1))
    
    # Combining the arrays 
    
    X_train = np.concatenate((W_dev, X_s_dev, A_dev[:, 1:]), axis=1)
    X_test = np.concatenate((W_test, X_s_test, A_test[:, 1:]), axis=1)
    
    
    print('Finished loading datasets')
    print('X_train Shape: ', X_train.shape)
    print('X_test Shape:' , X_test.shape)
    
    # scaling 
    
    standard_scaler = StandardScaler()
    
    X_train_scaled = np.around(standard_scaler.fit_transform(X_train), 2)
    X_test_scaled = np.around(standard_scaler.transform(X_test), 2)
    
    # producing final dataframes, not sure if I need these to be dataframes? It will slow everything down a lot 
    
    print("Data Scaling and Preprocessing Complete")
    
    return X_train_scaled, X_test_scaled, Y_dev, Y_test, train_units, test_units


import tensorflow as tf

# time window creation function
def temporaliser(X, y, units, time_window):
    
    engines = units[0].unique()
    
    first_engine_index = units[units[0]==engines[1]].index

    start = first_engine_index.min()
    end = first_engine_index.max() + 1

    # This is to allow us to line up the y variables with the time windows
    w = time_window - 1
    
    # this looks messy but we need to convert the dataframes into tensorflow arrays 
    
    first_X_dataset = tf.data.Dataset.from_tensor_slices(X[start:end, :])
    first_y_dataset = tf.data.Dataset.from_tensor_slices(y[start:end, :])
    
    first_temporalised = first_X_dataset.window(time_window, shift=1, drop_remainder=True)
    
    first_batched = first_temporalised.flat_map(lambda x:x.batch(time_window, drop_remainder=True))
    
    #y_tf = first_y_dataset
    
    full_combined_temporalised = tf.data.Dataset.zip((first_batched, first_y_dataset))
    
    # Now we can run the rest of the engines in a for loop and then concatenate them with the first engine 
    
    for engs in engines[1:]:
    
        engine_index = units[units[0]==engs].index

        start = engine_index.min()
        end = engine_index.max() + 1
        
        X_tf_array = tf.convert_to_tensor(X[start:end, :])
        y_tf_array = tf.convert_to_tensor(y[start:end, :])

        X_dataset = tf.data.Dataset.from_tensor_slices(X_tf_array)
        y_dataset = tf.data.Dataset.from_tensor_slices(y_tf_array)
    
        temporalised = X_dataset.window(time_window, shift=1, drop_remainder=True)
    
        batched = temporalised.flat_map(lambda x:x.batch(time_window, drop_remainder=True))
        
        combined_temporalised = tf.data.Dataset.zip((batched, y_dataset))
    
        full_combined_temporalised = full_combined_temporalised.concatenate(combined_temporalised)
    
    return full_combined_temporalised




import matplotlib.pyplot as plt
from matplotlib import gridspec
%matplotlib inline

# plot loss visualisation function
def plot_loss(fit_history, name):
    plt.figure(figsize=(13,5))
    plt.plot(range(1, len(fit_history.history['loss'])+1), fit_history.history['loss'], label='train')
    plt.plot(range(1, len(fit_history.history['val_loss'])+1), fit_history.history['val_loss'], label='validate')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.savefig(name)
    plt.show()
    
    
from sklearn.metrics import mean_squared_error, r2_score

# RMSE evaluation functoin
def evaluate(y_true, y_hat, label='test'):
    mse = mean_squared_error(y_true, y_hat)
    rmse = np.sqrt(mse)
    variance = r2_score(y_true, y_hat)
    print('{} set RMSE:{}, R2:{}'.format(label, rmse, variance))
