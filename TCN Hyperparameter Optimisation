from utils import *

# tensorflow stuff

import tensorflow as tf
from keras.layers import Dense, Input, BatchNormalisation, SpatialDropout1D, Convolution1D, GlobalMaxPooling, Activation
from keras import regularizers
from tensorflow.keras.optimizers import Adam

# imports needed for hyperparameter optimisation
from tensorflow.keras.wrappers.scikit_learn import KerasClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import KFold

# Creating our model wrapping function 

def create_model(padding = 'causal', dropout_rate = 0.1, nb_filters, Kernel_size, neurons, learning_rate):

    # the full model manually created. I have swapped dilation for strides, as we are creating a point estimation, it works exactly the same way, 
    but avoids having to complete full convolutions over the whole network for them just not to be used.

    inp = Input(shape=(243, 20))

    # initial layer
    x = Convolution1D(nb_filters, 1,  padding=padding, name='initial_conv_layer')(inp)

    # now we need 5 residual blocks 

    # Block 1

    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_1')(x)
    x = Activation('relu')(conv)
    x = BatchNormalization()(x)    
    x = SpatialDropout1D(dropout_rate)(x)
    x = Convolution1D(nb_filters, 1, padding='same')(x)

    # Block 2

    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_2')(x)
    x = Activation('relu')(conv)
    x = BatchNormalization()(x)    
    x = SpatialDropout1D(dropout_rate)(x)
    x = Convolution1D(nb_filters, 1, padding='same')(x)

    # Block 3

    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_3')(x)
    x = Activation('relu')(conv)
    x = BatchNormalization()(x)    
    x = SpatialDropout1D(dropout_rate)(x)
    x = Convolution1D(nb_filters, 1, padding='same')(x)

    # Block 4

    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_4')(x)
    x = Activation('relu')(conv)
    x = BatchNormalization()(x)    
    x = SpatialDropout1D(dropout_rate)(x)
    x = Convolution1D(nb_filters, 1, padding='same')(x)

    # Block 5

    conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_5')(x)
    x = Activation('relu')(conv)
    x = BatchNormalization()(x)    
    x = SpatialDropout1D(dropout_rate)(x)
    x = Convolution1D(nb_filters, 1, padding='same')(x)
    
    # Block 6 - optional 
    
    if window_length > 244:
      conv = Conv1D(filters=nb_filters, kernel_size=kernel_size, strides=kernel_size, padding=padding, name='Res_block_d')(x)
      x = Activation('relu')(conv)
      x = BatchNormalization()(x)    
      x = SpatialDropout1D(dropout_rate)(x)
      x = Convolution1D(nb_filters, 1, padding='same')(x)

    # pass it through one final relu activation function
    x = Activation('relu')(x)

    # due to the use of strides, this is not really needed except to remove a dimension
    conc = GlobalMaxPooling1D()(x)

    # now we can do our pooling
    conc = Dense(neaurons, activation="relu", kernel_regularizer=tf.keras.regularizers.L1(l1=0.000724))(conc)
    conc = Dropout(0.1)(conc)
    outp = Dense(1, activation="relu")(conc)

    opt = Adam(lr = learning_rate)

    # Compile your model with your optimizer, loss, and metrics
    
    model = Model(inputs=inp, outputs=outp)
    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    model.compile(loss='mse', optimizer=opt, metrics = [tf.keras.metrics.RootMeanSquaredError()])
    
    return model

# code to run the above 

# To save time we only ran hyperparameter optimisation on two of the datasets

filenames = ['N-CMAPSS_DS01.h5', 'N-CMAPSS_DS03-012.h5']

X_train_scaled, X_test_scaled, Y_dev, Y_test, train_units, test_units = loading_all_data(filenames)

training_ds_243 = temporaliser(X_train_scaled, Y_dev, train_units, 243)
test_ds_243 = temporaliser(X_test_scaled, Y_test, test_units, 243)

training = training_ds.shuffle(100000).batch(20).prefetch(tf.data.AUTOTUNE)
test = test_ds.shuffle(100000).batch(20).prefetch(tf.data.AUTOTUNE)
    
model = KerasClassifier(build_fn = create_model, verbose=1)

# Define the parameters to try out - you need to add a bunch more here
cv = KFold(n_splits=5)

params = {'nb_filters' = [64, 128],
          'kernel_size': [2, 3],
          'batch_size': [32, 64, 128], 
          'neurons' : [16, 34],
          'learning_rate': [0.01, 0.001], 
          'padding':['same'], 
          'epochs': [10]}

# Create a randomize search cv object passing in the parameters to try
random_search = RandomizedSearchCV(model, param_distributions = params, cv = cv, n_iter=60)

random_search.fit(input_X_train, output_Y_train)
              
# summarize result
print('Best Score: %s' % random_search.best_score_)
print('Best Hyperparameters: %s' % random_search.best_params_)

data = pd.DataFrame(random_search.cv_results_)

data.to_csv('Results.csv')
